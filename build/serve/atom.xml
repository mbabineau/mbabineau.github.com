<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>mikebabineau</title>
 <link href="http://mbabineau.github.com/atom.xml" rel="self"/>
 <link href="http://mbabineau.github.com"/>
 <updated>2013-08-21T15:32:20-07:00</updated>
 <id>http://mbabineau.github.com</id>
 <author>
   <name>Mike Babineau</name>
   <email>michael.babineau@gmail.com</email>
 </author>

 
 <entry>
   <title>Multi Region Gotcha on Elastic Beanstalk</title>
   <link href="http://mbabineau.github.com/2013/08/21/multi-region-gotcha-on-elastic-beanstalk"/>
   <updated>2013-08-21T00:00:00-07:00</updated>
   <id>http://mbabineau.github.com/2013/08/21/multi-region-gotcha-on-elastic-beanstalk</id>
   <content type="html">&lt;p&gt;It&amp;#8217;s not in the current Elastic Beanstalk documentation, but you can&amp;#8217;t create a new application version from an S3 file hosted in a different region. Attempts to do so will return this error:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;aws elasticbeanstalk create-application-version --region ap-southeast-1 --application-name myapp --version-label myversion --source-bundle &lt;span class='s1'&gt;&amp;#39;{&amp;quot;S3Bucket&amp;quot;:&amp;quot;mybuilds&amp;quot;, &amp;quot;S3Key&amp;quot;:&amp;quot;myapp-myversion.war&amp;quot;}&amp;#39;&lt;/span&gt;
&lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='s2'&gt;&amp;quot;Errors&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;[&lt;/span&gt;
        &lt;span class='o'&gt;{&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;Message&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;Unable to download from S3 location (Bucket: mybuilds  Key: myapp-myversion.war). Reason: Moved Permanently&amp;quot;&lt;/span&gt;, 
            &lt;span class='s2'&gt;&amp;quot;Code&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;InvalidParameterCombination&amp;quot;&lt;/span&gt;, 
            &lt;span class='s2'&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;Sender&amp;quot;&lt;/span&gt;
        &lt;span class='o'&gt;}&lt;/span&gt;
    &lt;span class='o'&gt;]&lt;/span&gt;, 
    &lt;span class='s2'&gt;&amp;quot;ApplicationVersion&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{}&lt;/span&gt;, 
    &lt;span class='s2'&gt;&amp;quot;ResponseMetadata&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
        &lt;span class='s2'&gt;&amp;quot;RequestId&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;bfaf70b6-0aae-11e3-ae62-0d8638135266&amp;quot;&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you want to create an application version in multiple regions, you&amp;#8217;ll need a location-constrained bucket for each region. It&amp;#8217;s a good pattern to include the region in the bucket name:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;aws s3 create-bucket --bucket mybuilds-ap-southeast-1 --create-bucket-configuration &lt;span class='s1'&gt;&amp;#39;{&amp;quot;LocationConstraint&amp;quot;:&amp;quot;ap-southeast-1&amp;quot;}&amp;#39;&lt;/span&gt;
&lt;span class='nv'&gt;$ &lt;/span&gt;aws s3 create-bucket --bucket mybuilds-eu-west-1 --create-bucket-configuration &lt;span class='s1'&gt;&amp;#39;{&amp;quot;LocationConstraint&amp;quot;:&amp;quot;eu-west-1&amp;quot;}&amp;#39;&lt;/span&gt;
&lt;span class='nv'&gt;$ &lt;/span&gt;aws s3 create-bucket --bucket mybuilds-sa-east-1 --create-bucket-configuration &lt;span class='s1'&gt;&amp;#39;{&amp;quot;LocationConstraint&amp;quot;:&amp;quot;sa-east-1&amp;quot;}&amp;#39;&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, just use the regional bucket for each &lt;code&gt;create-application-version&lt;/code&gt; call:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;aws elasticbeanstalk create-application-version --region ap-southeast-1 --application-name myapp --version-label myversion --source-bundle &lt;span class='s1'&gt;&amp;#39;{&amp;quot;S3Bucket&amp;quot;:&amp;quot;mybuilds-ap-southeast-1&amp;quot;, &amp;quot;S3Key&amp;quot;:&amp;quot;myapp-myversion.war&amp;quot;}&amp;#39;&lt;/span&gt;
&lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='s2'&gt;&amp;quot;ApplicationVersion&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
        &lt;span class='s2'&gt;&amp;quot;ApplicationName&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;myapp&amp;quot;&lt;/span&gt;, 
        &lt;span class='s2'&gt;&amp;quot;VersionLabel&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;myversion&amp;quot;&lt;/span&gt;, 
        &lt;span class='s2'&gt;&amp;quot;SourceBundle&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;S3Bucket&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;mybuilds-ap-southeast-1&amp;quot;&lt;/span&gt;, 
            &lt;span class='s2'&gt;&amp;quot;S3Key&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;myapp-myversion.war&amp;quot;&lt;/span&gt;
        &lt;span class='o'&gt;}&lt;/span&gt;, 
        &lt;span class='s2'&gt;&amp;quot;DateUpdated&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;2013-08-21T22:12:32.738Z&amp;quot;&lt;/span&gt;, 
        &lt;span class='s2'&gt;&amp;quot;DateCreated&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;2013-08-21T22:12:32.738Z&amp;quot;&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
 </entry>
 
 <entry>
   <title>Icinga-PagerDuty Integration via Chef</title>
   <link href="http://mbabineau.github.com/2013/03/04/chef-recipe-for-icinga-pagerduty-integration"/>
   <updated>2013-03-04T00:00:00-08:00</updated>
   <id>http://mbabineau.github.com/2013/03/04/chef-recipe-for-icinga-pagerduty-integration</id>
   <content type="html">&lt;p&gt;I wrote a Chef recipe for enabling PagerDuty support in Icinga. With luck, it&amp;#8217;ll be merged into Marius Ducea&amp;#8217;s &lt;a href='https://github.com/mdxp/icinga-cookbook'&gt;icinga cookbook&lt;/a&gt;. The code is &lt;a href='https://github.com/mdxp/icinga-cookbook/pull/11'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id='usage'&gt;Usage&lt;/h3&gt;

&lt;h4 id='configure_pagerduty'&gt;Configure PagerDuty&lt;/h4&gt;

&lt;p&gt;Add a service for Icinga:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Go to &lt;a href='https://your-domain.pagerduty.com/services/new'&gt;https://your-domain.pagerduty.com/services/new&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Set the service type to &amp;#8220;Nagios&amp;#8221;&lt;/li&gt;

&lt;li&gt;Add the service&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id='configure_your_monitoring_node'&gt;Configure your monitoring node&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Add the &lt;code&gt;icinga::pagerduty&lt;/code&gt; recipe to your role&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; name &amp;quot;monitoring&amp;quot;
 description &amp;quot;Monitoring server&amp;quot;
 run_list(
   &amp;quot;recipe[icinga]&amp;quot;,
   &amp;quot;recipe[icinga::pagerduty]&amp;quot;
 )    &lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;Get the new PagerDuty service&amp;#8217;s API key &lt;img alt='api key' src='/img/pagerduty_service_api_key.png' /&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;Copy it into your node attributes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; default_attributes({
   :icinga =&amp;gt; {
     :pagerduty =&amp;gt; {
       :service_key =&amp;gt; &amp;quot;318e318e318e318e318e318e318ead29cf&amp;quot;
     }
 })&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;Run chef-client&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id='watch_alerts_show_up_in_pagerduty'&gt;Watch alerts show up in PagerDuty&lt;/h4&gt;

&lt;p&gt;You should now see alerts like these: &lt;img alt='api key' src='/img/pagerduty_icinga_alert.png' /&gt;&lt;/p&gt;

&lt;p&gt;PagerDuty will automatically resolve these incidents as Icinga sends recovery notifications. More details &lt;a href='http://www.pagerduty.com/docs/nagios-integration-guide/'&gt;here&lt;/a&gt;.&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>No, I will not fix your printer (and that's a good thing)</title>
   <link href="http://mbabineau.github.com/2013/02/10/no-i-will-not-fix-your-printer"/>
   <updated>2013-02-10T00:00:00-08:00</updated>
   <id>http://mbabineau.github.com/2013/02/10/no-i-will-not-fix-your-printer</id>
   <content type="html">&lt;p&gt;Don&amp;#8217;t confuse ops with office IT. Hire someone else for that.&lt;/p&gt;

&lt;p&gt;In a growing startup, it&amp;#8217;s common to pawn off tasks like setting up the new hire&amp;#8217;s laptop or fixing the printer to your ops guy. After all, that&amp;#8217;s what sysadmins do, right?&lt;/p&gt;

&lt;p&gt;Wrong. You hired your ops guy to run your site&amp;#8217;s infrastructure. Time spent on fixing printers is time &lt;em&gt;not&lt;/em&gt; spent on monitoring, logging, automation, or countless other systems critical to keeping your site up and running. Having your ops guy figure out why Jim&amp;#8217;s machine &amp;#8220;feels slow&amp;#8221; is as reasonable as having your UI designer repaint the office.&lt;/p&gt;

&lt;p&gt;All that said, broken printers and slow laptops &lt;em&gt;are&lt;/em&gt; disruptive. Office IT is an important business function, so hire someone to own it.&lt;/p&gt;

&lt;p&gt;But what if you can&amp;#8217;t justify the headcount? Contract it out. Put someone on retainer for 8 hours a week, have them come in every Tuesday and Thursday afternoon. It&amp;#8217;s cheaper than another ops guy, minimizes context switching, and office IT problems won&amp;#8217;t be de-prioritized to oblivion.&lt;/p&gt;

&lt;p&gt;Find someone who does this for a living. Your ops guy and your printer will thank you.&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Delay Queues in Redis (with Grails example)</title>
   <link href="http://mbabineau.github.com/2013/02/09/delay-queues-in-redis"/>
   <updated>2013-02-09T00:00:00-08:00</updated>
   <id>http://mbabineau.github.com/2013/02/09/delay-queues-in-redis</id>
   <content type="html">&lt;p&gt;I needed a way to handle delayed processing of messages in a distributed system. Since I already had Redis running (but no message queue other than Kafka), I used a sorted set as a simple delay queue.&lt;/p&gt;

&lt;p&gt;The basic approach is to insert each message into the sorted set with a score equal to the [unix] time the message should become available for processing (&amp;#8220;ready&amp;#8221;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis&amp;gt; ZADD delayqueue &amp;lt;future_timestamp&amp;gt; &amp;quot;messsage&amp;quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get all &amp;#8220;ready&amp;#8221; messages with a range query from (time) zero to now, then delete the messages. To avoid multiple processing and lost messages, run this in a transaction:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;redis&amp;gt; MULTI
redis&amp;gt; ZRANGEBYSCORE delayqueue 0 &amp;lt;current_timestamp&amp;gt;
redis&amp;gt; ZREMRANGEBYSCORE delayqueue 0 &amp;lt;current_timestamp&amp;gt;
redis&amp;gt; EXEC&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that this implementation requires messages to be unique per queue.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s a quick implementation as a Grails service:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='groovy'&gt;&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;redis.clients.jedis.Jedis&lt;/span&gt;

&lt;span class='cm'&gt;/**&lt;/span&gt;
&lt;span class='cm'&gt; * Handles the delaying of queued messages for later retrieval.&lt;/span&gt;
&lt;span class='cm'&gt; */&lt;/span&gt;
&lt;span class='kd'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;DelayQueueService&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='kt'&gt;def&lt;/span&gt; &lt;span class='n'&gt;redisService&lt;/span&gt;
    
    &lt;span class='cm'&gt;/**&lt;/span&gt;
&lt;span class='cm'&gt;     * Queue a message for later retrieval. Messages are unique per queue and &lt;/span&gt;
&lt;span class='cm'&gt;     * are deleted upon retrieval. If a given message already exists, it is &lt;/span&gt;
&lt;span class='cm'&gt;     * updated with the new delay.&lt;/span&gt;
&lt;span class='cm'&gt;     *&lt;/span&gt;
&lt;span class='cm'&gt;     * @param queue Queue name&lt;/span&gt;
&lt;span class='cm'&gt;     * @param message&lt;/span&gt;
&lt;span class='cm'&gt;     * @param delay Time in seconds the message should be delayed&lt;/span&gt;
&lt;span class='cm'&gt;     */&lt;/span&gt;
    &lt;span class='kt'&gt;def&lt;/span&gt; &lt;span class='nf'&gt;queueMessage&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;String&lt;/span&gt; &lt;span class='n'&gt;queue&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;String&lt;/span&gt; &lt;span class='n'&gt;message&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;Integer&lt;/span&gt; &lt;span class='n'&gt;delay&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
        &lt;span class='kt'&gt;def&lt;/span&gt; &lt;span class='n'&gt;time&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;System&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;currentTimeMillis&lt;/span&gt;&lt;span class='o'&gt;()&lt;/span&gt;&lt;span class='s'&gt;/1000 + delay&lt;/span&gt;

&lt;span class='s'&gt;        redisService.withRedis { Jedis redis -&amp;gt;&lt;/span&gt;
&lt;span class='s'&gt;            redis.zadd(queue, time, message)&lt;/span&gt;
&lt;span class='s'&gt;        }&lt;/span&gt;
&lt;span class='s'&gt;    }&lt;/span&gt;

&lt;span class='s'&gt;    /&lt;/span&gt;&lt;span class='o'&gt;**&lt;/span&gt;
     &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;Retrieve&lt;/span&gt; &lt;span class='n'&gt;messages&lt;/span&gt; &lt;span class='n'&gt;that&lt;/span&gt; &lt;span class='n'&gt;are&lt;/span&gt; &lt;span class='n'&gt;no&lt;/span&gt; &lt;span class='n'&gt;longer&lt;/span&gt; &lt;span class='n'&gt;delayed&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt; &lt;span class='n'&gt;Deletes&lt;/span&gt; &lt;span class='n'&gt;messages&lt;/span&gt; &lt;span class='n'&gt;on&lt;/span&gt; &lt;span class='n'&gt;read&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;
     &lt;span class='o'&gt;*&lt;/span&gt;
     &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='nd'&gt;@param&lt;/span&gt; &lt;span class='n'&gt;queue&lt;/span&gt; &lt;span class='n'&gt;Queue&lt;/span&gt; &lt;span class='n'&gt;name&lt;/span&gt;
     &lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='s'&gt;/&lt;/span&gt;
&lt;span class='s'&gt;    def getMessages(String queue) {&lt;/span&gt;
&lt;span class='s'&gt;        def startTime = 0&lt;/span&gt;
&lt;span class='s'&gt;        def endTime = System.currentTimeMillis() /&lt;/span&gt; &lt;span class='mi'&gt;1000&lt;/span&gt;

        &lt;span class='n'&gt;redisService&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;withRedis&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt; &lt;span class='n'&gt;Jedis&lt;/span&gt; &lt;span class='n'&gt;redis&lt;/span&gt; &lt;span class='o'&gt;-&amp;gt;&lt;/span&gt;
            &lt;span class='kt'&gt;def&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;redis&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;multi&lt;/span&gt;&lt;span class='o'&gt;()&lt;/span&gt;
            &lt;span class='kt'&gt;def&lt;/span&gt; &lt;span class='n'&gt;response&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;zrangeByScore&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;queue&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;startTime&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;endTime&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
            &lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;zremrangeByScore&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;queue&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;startTime&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;endTime&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
            &lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;exec&lt;/span&gt;&lt;span class='o'&gt;()&lt;/span&gt;
            &lt;span class='n'&gt;response&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;get&lt;/span&gt;&lt;span class='o'&gt;()&lt;/span&gt;
        &lt;span class='o'&gt;}&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
 </entry>
 
 <entry>
   <title>EA2D Repo Relocation</title>
   <link href="http://mbabineau.github.com/2013/02/08/ea2d-repo-relocation"/>
   <updated>2013-02-08T00:00:00-08:00</updated>
   <id>http://mbabineau.github.com/2013/02/08/ea2d-repo-relocation</id>
   <content type="html">&lt;p&gt;Since it looks like EA2D&amp;#8217;s GitHub account has been deleted (the group doesn&amp;#8217;t exist anymore), I&amp;#8217;ve moved the (kinda) maintained versions of several open source projects to my personal account.&lt;/p&gt;

&lt;p&gt;The includes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/mbabineau/loggly-python'&gt;pingdom-python&lt;/a&gt; - Python library for Pingdom&amp;#8217;s REST API&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/mbabineau/loggly-python'&gt;loggly-python&lt;/a&gt; - Python library for Loggly&amp;#8217;s REST API&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/mbabineau/loggly-python'&gt;loggly-cookbook&lt;/a&gt; - Chef coookbook for Loggly&lt;/li&gt;
&lt;/ul&gt;</content>
 </entry>
 
 <entry>
   <title>Chef Cookbook for Loggly</title>
   <link href="http://mbabineau.github.com/2011/04/07/loggly-chef-cookbook"/>
   <updated>2011-04-07T00:00:00-07:00</updated>
   <id>http://mbabineau.github.com/2011/04/07/loggly-chef-cookbook</id>
   <content type="html">&lt;p&gt;&lt;em&gt;(originally written for EA2D&amp;#8217;s engineering blog)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As mentioned in a &lt;a href='http://eng.ea2d.com/loggly-python-an-open-source-library'&gt;previous post&lt;/a&gt;, we aggregate and store our logs using a service called &lt;a href='http://loggly.com'&gt;Loggly&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since we wrote a &lt;a href='https://github.com/mbabineau/loggly-python'&gt;library&lt;/a&gt; for programmatically managing Loggly inputs and devices, it was only natural for us to integrate it with our &lt;a href='http://wiki.opscode.com/display/chef/Home'&gt;Chef&lt;/a&gt; deployment.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ve written and open sourced a Chef cookbook for Loggly.&lt;/p&gt;

&lt;p&gt;From the &lt;a href='https://github.com/mbabineau/loggly-cookbook#readme'&gt;README&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Installs the loggly-python library and provides a definition for the configuration of Loggly logging.&lt;/p&gt;

&lt;p&gt;More specifically, the loggly_conf definition will configure rsyslog to watch a log file and send its lines to a Loggly input. When first run, loggly_conf will create the input and authorize the server to publish events to that input.&lt;/p&gt;

&lt;p&gt;Developed for and tested on Ubuntu 10.10 LTS&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can grab it from the Opscode site:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ knife cookbook site vendor loggly&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or from our GitHub repository:&lt;/p&gt;

&lt;p&gt;&lt;a href='https://github.com/mbabineau/loggly-cookbook'&gt;https://github.com/mbabineau/loggly-cookbook&lt;/a&gt;&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Continuous Deployment of Ops Configs</title>
   <link href="http://mbabineau.github.com/2011/04/06/continuous-deployment-of-ops-configs"/>
   <updated>2011-04-06T00:00:00-07:00</updated>
   <id>http://mbabineau.github.com/2011/04/06/continuous-deployment-of-ops-configs</id>
   <content type="html">&lt;p&gt;A core tenant of DevOps is the notion of &amp;#8220;Infrastructure as Code.&amp;#8221; Provisioning and deployment should be done programmatically, potentially with a configuration management (CM) system such as Chef or Puppet.&lt;/p&gt;

&lt;p&gt;Jesse Robbins describes the goal well:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“Enable the reconstruction of the business from nothing but a source code repository, an application data backup, and bare metal resources”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;While CM tools facilitate this reconstruction, there are some tricks for getting the most of your implementation. Below are the key points from a &lt;a href='http://www.slideshare.net/mbabineau/continuous-deployment-for-ops-who-use-chef-and-git'&gt;lightning talk&lt;/a&gt; I gave on this at the &lt;a href='http://www.meetup.com/ArchCamp/events/16419122/'&gt;last ArchCamp&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id='the_wrong_way'&gt;The wrong way&lt;/h3&gt;

&lt;p&gt;Using the configuration management server as the system of record. A Chef example is creating and modifying roles directly on the Chef server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ knife node create myrole
$ knife node edit myrole&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This decreases visibility and introduces unnecessary risk. Losing the Chef server is now a major event. Yes, you can mitigate this risk with regular backups, but you still lack visibility into changes.&lt;/p&gt;

&lt;h3 id='a_better_approach'&gt;A better approach&lt;/h3&gt;

&lt;p&gt;Place your CM data into source control and treat that as your system of record. Changes are committed to source control, then deployed to the CM server. Under Jesse&amp;#8217;s description, this buckets CM configs as source code rather than application data.&lt;/p&gt;

&lt;p&gt;Going back to the Chef example, your workflow would instead look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git commit -am &amp;quot;added newfeature to myrole&amp;quot;
$ knife role from file roles/myrole.json&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gives you an auditable history of every operational config change.&lt;/p&gt;

&lt;h3 id='deploying_via_git'&gt;Deploying via git&lt;/h3&gt;

&lt;p&gt;But what if instead of deploying Chef changes via knife, you did so using git? Changes to multiple roles or cookbooks could be pushed simultaneously, and with one command.&lt;/p&gt;

&lt;p&gt;In other words, this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git commit -am &amp;quot;added newfeature to mycookbook, integrated it with myrole, and added supporting data to mydatabag&amp;quot;
$ knife cookbook upload mycookbook
$ knife role from file roles/myrole.json
$ knife data bag from file mydatabag newitem&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Would become:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git commit -am &amp;quot;added newfeature to mycookbook, integrated it with myrole, and added supporting data to mydatabag&amp;quot;
$ git push origin master&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This type of deployment can be implemented via a process that monitors the git repo for changes and deploys any changes to the Chef server.&lt;/p&gt;

&lt;p&gt;A simplified version of the upload code is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!bash
for cookbook in $(git_diff(&amp;quot;cookbooks&amp;quot;)); do
    knife cookbook upload $cookbook
done

for role in $(git_diff(&amp;quot;roles&amp;quot;)); do
    knife role from file $role
done

for bag in $(git_diff(&amp;quot;databags&amp;quot;)); do
    for item in $(git_diff(&amp;quot;items&amp;quot;, $bag)); do
        knife data bag from file $bag $item
    done
done&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By putting this on a continuous integration server (Jenkins, BuildBot, etc.) and detecting repository changes through polling or post-commit hooks, you implement continuous deployment of your operational configs.&lt;/p&gt;

&lt;p&gt;That wasn&amp;#8217;t so bad, was it?&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t forget to &lt;a href='http://www.slideshare.net/mbabineau/continuous-deployment-for-ops-who-use-chef-and-git'&gt;check out the slides&lt;/a&gt;.&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>pingdom-python</title>
   <link href="http://mbabineau.github.com/2011/02/08/pingdom-python"/>
   <updated>2011-02-08T00:00:00-08:00</updated>
   <id>http://mbabineau.github.com/2011/02/08/pingdom-python</id>
   <content type="html">&lt;p&gt;&lt;a href='http://pingdom.com'&gt;Pingdom&lt;/a&gt; is one of several monitoring tools we use at EA2D. Besides alerting us when things go down, we query Pingdom&amp;#8217;s API to include check status in our dashboards.&lt;/p&gt;

&lt;p&gt;The old Pingdom SOAP API was unwieldy and slow. Fortunately, Pingdom &lt;a href='http://royal.pingdom.com/2011/03/22/new-pingdom-api-enters-public-beta/'&gt;released&lt;/a&gt; a new, JSON-ified REST API that remedied the problems of its predecessor.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve written a Python library for this new API and released it as open source. For now, it supports only a subset of available resources, but the framework is there for others to be added easily.&lt;/p&gt;

&lt;p&gt;&lt;a href='https://github.com/mbabineau/pingdom-python'&gt;https://github.com/mbabineau/pingdom-python&lt;/a&gt;&lt;/p&gt;

&lt;h3 id='pingdompython_in_action'&gt;pingdom-python in action&lt;/h3&gt;

&lt;p&gt;Set up a Pingdom connection:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import pingdom
&amp;gt;&amp;gt;&amp;gt; c = pingdom.PingdomConnection(PINGDOM_USERNAME, PINGDOM_PASSWORD)  # Same credentials you use for the Pingdom website&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a new Pingdom check:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; c.create_check(&amp;#39;EA2D Website&amp;#39;, &amp;#39;ea2d.com&amp;#39;, &amp;#39;http&amp;#39;)
Check:EA2D Website&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get basic information about a Pingdom check:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; check = c.get_all_checks([&amp;#39;EA2D Website&amp;#39;])[0]   # Expects a list, returns a list
&amp;gt;&amp;gt;&amp;gt; check.id
302632
&amp;gt;&amp;gt;&amp;gt; check.status
u&amp;#39;up&amp;#39;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get more detailed information about a Pingdom check:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; check = c.get_check(210702)  # Look up by check ID
&amp;gt;&amp;gt;&amp;gt; check.lasterrortime
1289482981&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete a Pingdom check:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; c.delete_check(302632)
{u&amp;#39;message&amp;#39;: u&amp;#39;Deletion of check was successful!&amp;#39;}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Go check it out on &lt;a href='https://github.com/mbabineau/pingdom-python'&gt;GitHub&lt;/a&gt;, or install it directly from PyPI:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo easy_install pingdom&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have any questions, drop me a line: &lt;a href='mailto:michael.babineau@gmail.com'&gt;michael.babineau@gmail.com&lt;/a&gt;.&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>loggly-python</title>
   <link href="http://mbabineau.github.com/2011/01/20/loggly-python"/>
   <updated>2011-01-20T00:00:00-08:00</updated>
   <id>http://mbabineau.github.com/2011/01/20/loggly-python</id>
   <content type="html">&lt;p&gt;&lt;em&gt;(originally written for EA2D&amp;#8217;s engineering blog)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For centralized logging, we use a service called &lt;a href='http://loggly.com'&gt;Loggly&lt;/a&gt;. We forward our logs to Loggly and aggregate them by application and environment. This gives us a handy web interface for viewing logs across all servers within an application group, and provides some great tools for search, comparison, and alerting.&lt;/p&gt;

&lt;p&gt;We send events to Loggly using syslog over TCP, and these events are bucketed based on destination port. For security, Loggly locks down each port to a list of authorized IP addresses.&lt;/p&gt;

&lt;p&gt;Since we make heavy use of EC2 &lt;a href='http://aws.amazon.com/autoscaling/'&gt;Auto Scaling Groups&lt;/a&gt;, we simply can not maintain this authorized IP list manually. Additionally, we&amp;#8217;re constantly launching new applications and environments, so our list of buckets (Loggly &amp;#8220;inputs&amp;#8221;) is in constant flux.&lt;/p&gt;

&lt;p&gt;Fortunately, Loggly has exposed a set of administration &lt;a href='http://wiki.loggly.com/apidocumentation'&gt;APIs&lt;/a&gt; for managing inputs and authorized devices. Since no library was available, we ended up writing one ourselves (in Python) and releasing it as open source.&lt;/p&gt;

&lt;h3 id='getting_the_library'&gt;Getting the library&lt;/h3&gt;

&lt;p&gt;You can find it on GitHub:&lt;/p&gt;

&lt;p&gt;&lt;a href='https://github.com/mbabineau/loggly-python'&gt;https://github.com/mbabineau/loggly-python&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Or install it from PyPI:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo easy_install loggly&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='batteriesincluded'&gt;Batteries-included&lt;/h3&gt;

&lt;p&gt;This package includes scripts for managing inputs and devices. To use them, simply set up your credentials:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export LOGGLY_USERNAME=&amp;#39;someuser&amp;#39;
export LOGGLY_PASSWORD=&amp;#39;somepassword&amp;#39;
export LOGGLY_DOMAIN=&amp;#39;somesubdomain.loggly.com&amp;#39;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create an input:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ loggly-create-input -i testinput -s syslogtcp
Creating input &amp;quot;testinput&amp;quot; of type &amp;quot;syslogtcp&amp;quot;
Input:testinput2&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add a device to an input:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ loggly-add-device -i testinput -d 192.168.1.1
Adding device &amp;quot;192.168.1.1&amp;quot; to input &amp;quot;testinput&amp;quot;
Device:192.168.1.1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete a device:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ loggly-remove-device -d 192.168.1.1
Removing device &amp;quot;192.168.1.1&amp;quot; from all inputs&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete an input:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ loggly-delete-input -i testinput
Deleting input testinput&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>GSLB with EC2</title>
   <link href="http://mbabineau.github.com/2010/05/08/gslb-with-ec2"/>
   <updated>2010-05-08T00:00:00-07:00</updated>
   <id>http://mbabineau.github.com/2010/05/08/gslb-with-ec2</id>
   <content type="html">&lt;p&gt;&lt;em&gt;(originally posted on Bizo&amp;#8217;s dev blog &lt;a href='http://dev.bizo.com/2010/05/improving-global-application.html'&gt;here&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is an unofficial continuation of Amazon&amp;#8217;s &lt;a href='http://aws.typepad.com/aws/2010/05/improving-global-application-performance.html'&gt;blog post&lt;/a&gt; on the use of Amazon CloudFront to improve application performance.&lt;/p&gt;

&lt;p&gt;CloudFront is a great CDN to consider, especially if you&amp;#8217;re already an Amazon Web Services customer. Unfortunately, it can only be used for static content; the loading of dynamic content will still be slower for far-away users than for nearby ones. Simply put, users in India will still see a half-second delay when loading the dynamic portions of your US-based website. And a half-second delay has a &lt;a href='http://glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html'&gt;measurable impact on revenue&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s talk about speeding up dynamic content, globally.&lt;/p&gt;

&lt;p&gt;The typical EC2 implementation comprises instances deployed in a single region. Such a deployment may span several availability zones for redundancy, but all instances are in roughly the same place, geographically.&lt;/p&gt;

&lt;p&gt;This is fine for EC2-hosted apps with nominal revenue or a highly localized user base. But what if your users are spread around the globe? The problem can&amp;#8217;t be solved by moving your application to another region - that would simply shift the extra latency to another group.&lt;/p&gt;

&lt;p&gt;For a distributed audience, you need a distributed infrastructure. But you can&amp;#8217;t simply launch servers around the world and expect traffic to reach them. Enter Global Server Load Balancing (GSLB).&lt;/p&gt;

&lt;h3 id='a_primer_on_gslb'&gt;A primer on GSLB&lt;/h3&gt;

&lt;p&gt;Broadly, GSLB is used to intelligently distribute traffic across multiple datacenters based on some set of rules.&lt;/p&gt;

&lt;p&gt;With GSLB, your traffic distribution can go from this: &lt;img alt='without gslb' src='/img/gslb-map-without_gslb.gif' /&gt;&lt;/p&gt;

&lt;p&gt;To this: &lt;img alt='with gslb' src='/img/gslb-map-with_gslb.gif' /&gt;&lt;/p&gt;

&lt;p&gt;GSLB can be implemented as a feature of a physical device (including certain high-end load balancers) or as a part of a DNS service. Since we EC2 users are clearly not interested in hardware, our focus is on the latter: DNS-based GSLB.&lt;/p&gt;

&lt;p&gt;Standard DNS behavior is for an authoritative nameserver to, given queries for a certain record, always return the same result. A DNS-based implementation of GSLB would alter this behavior so that queries return context-dependent results.&lt;/p&gt;

&lt;p&gt;Example:&lt;br /&gt;User A queries DNS for gslb.example.com &amp;#8211; response: 10.1.0.1&lt;br /&gt;User B queries DNS for gslb.example.com &amp;#8211; response: 10.2.0.1&lt;/p&gt;

&lt;p&gt;But what context should we use? Since our goal is to reduce wire latency, we should route users to the closest datacenter. IP blocks can be mapped geographically &amp;#8211; by examining a requestor&amp;#8217;s IP address, a GSLB service can return a geo-targeted response.&lt;/p&gt;

&lt;p&gt;With geo-targeted DNS, our example would be:&lt;br /&gt;User A (in China) queries DNS for geo.example.com &amp;#8211; response: 10.1.0.1&lt;br /&gt;User B (in Spain) queries DNS for geo.example.com &amp;#8211; response: 10.2.0.1&lt;/p&gt;

&lt;h3 id='getting_started'&gt;Getting started&lt;/h3&gt;

&lt;p&gt;At a high level, implementation can be broken down into two steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Deploy infrastructure in other AWS regions&lt;/li&gt;

&lt;li&gt;Configure GSLB-capable DNS&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Infrastructure configurations will vary from shop to shop, but as an example, a read-heavy EC2 application with a single master database for writes should:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;deploy application servers to all regions&lt;/li&gt;

&lt;li&gt;deploy read-only (slave) database servers and/or read caches to all regions&lt;/li&gt;

&lt;li&gt;configure application servers to use the slave database servers and/or read caches in their region for reads&lt;/li&gt;

&lt;li&gt;configure application servers to use the single master in the &amp;#8220;main&amp;#8221; region for writes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is what such an environment would look like: &lt;img alt='architecture' src='/img/gslb-architecture.gif' /&gt;&lt;/p&gt;

&lt;p&gt;When configuring servers to communicate across regions (app servers -&amp;gt; master DB; slave DBs -&amp;gt; master DB), you will need to use IP-based rules for your security groups; traffic from the &amp;#8220;app-servers&amp;#8221; security group you set up in eu-west-1 is indistinguishable from other traffic to your DB server in us-east-1. This is because cross-region communication is done using external IP addresses. Your best bet is to either automate security group updates or use Elastic IPs.&lt;/p&gt;

&lt;p&gt;Note on more complex configurations: distributed backends are hard (see &lt;a href='http://en.wikipedia.org/wiki/CAP_theorem'&gt;Brewer&amp;#8217;s [CAP] theorem&lt;/a&gt;). Multi-region EC2 environments are much easier to implement if your application tolerates the use of 1) regional caches for reads; 2) centralized writes. If you have a choice, stick with the simpler route.&lt;/p&gt;

&lt;p&gt;As for configuring DNS, several companies have DNS-based GSLB service offerings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='http://dyn.com/dynect'&gt;Dynect&lt;/a&gt; - &lt;a href='http://dyn.com/dynect-traffic-management'&gt;Traffic Management&lt;/a&gt; (A records only) and &lt;a href='http://dyn.com/dynect-cdn-manager'&gt;CDN Manager&lt;/a&gt; (CNAMEs allowed)&lt;/li&gt;

&lt;li&gt;&lt;a href='http://www.akamai.com/'&gt;Akamai&lt;/a&gt; - &lt;a href='http://www.akamai.com/html/technology/products/gtm.html'&gt;Global Traffic Management&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='http://www.ultradns.com/'&gt;UltraDNS&lt;/a&gt; - &lt;a href='http://www.ultradns.com/solutions/directionaldns.html'&gt;Directional DNS&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='http://comwired.com/'&gt;Comwired&lt;/a&gt;/&lt;a href='http://www.dns.com/'&gt;DNS.com&lt;/a&gt; - &lt;a href='http://www.dns.com/location/'&gt;Location Geo-Targeting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DNS configuration should be pretty similar for the vendors listed above. Basic steps are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;set up regional CNAMEs (us-east-1.example.com, us-west-1.example.com, eu-west-1.example.com, ap-southeast-1.example.com)&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;set up a GSLB-enabled &amp;#8220;master&amp;#8221; CNAME (www.example.com)&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;define the GSLB rules:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For users in Asia, return ap-southeast-1.example.com&lt;/li&gt;

&lt;li&gt;For users in Europe, return eu-west-1.example.com&lt;/li&gt;

&lt;li&gt;For users in Western US, return us-west-1.example.com - &amp;#8230;&lt;/li&gt;

&lt;li&gt;For all other users, return us-east-1.example.com&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If your application is already live, consider abstracting the DNS records by one layer: geo.example.com (master record); us-east-1.geo.example.com, us-west-1.geo.example.com, etc. (regional records). Bring the new configuration live by pointing www.example.com (CNAME) to geo.example.com.&lt;/p&gt;

&lt;h3 id='bizos_experiences'&gt;Bizo&amp;#8217;s experiences&lt;/h3&gt;

&lt;p&gt;Several of our EC2 applications serve embedded content for customer websites, so it&amp;#8217;s critical we minimize load times. Here&amp;#8217;s the difference we saw on one app after expanding into new regions (from us-east-1 to us-east-1, us-west-1, and eu-west-1) and implementing GSLB (load times provided by &lt;a href='http://browsermob.com/'&gt;BrowserMob&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;Load times before GSLB: &lt;img alt='before' src='/img/gslb-response-before.png' /&gt;&lt;/p&gt;

&lt;p&gt;Load times after GSLB: &lt;img alt='after' src='/img/gslb-response-before.png' /&gt;&lt;/p&gt;

&lt;p&gt;Reduced load times for everyone far from us-east-1. Users are happy, customers are happy, we&amp;#8217;re happy. Overall, a success.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s interesting to see how the load is distributed throughout the day. Here&amp;#8217;s one application&amp;#8217;s HTTP traffic, broken down by region (ELB stats graphed by &lt;a href='https://github.com/mbabineau/cloudviz'&gt;cloudviz&lt;/a&gt;): &lt;img alt='graph' src='/img/gslb-stats.png' /&gt;&lt;/p&gt;

&lt;p&gt;Note that the use of Elastic Load Balancers and Auto Scaling becomes much more compelling with GSLB. By geographically partitioning users, peak hours are much more localized. This results in a wider difference between peak and trough demand per region; Auto Scaling adjusts capacity transparently, reducing the marginal cost of expanding your infrastructure to multiple AWS regions.&lt;/p&gt;

&lt;p&gt;For our GSLB DNS service, we use &lt;a href='http://dyn.com/dynectsales'&gt;Dynect&lt;/a&gt; and couldn&amp;#8217;t be more pleased. Intuitive management interface, responsive and helpful support, friendly, no-BS sales. Pricing is based on number of GSLB-enabled domains and DNS query rate. Contact Dynect sales if you want specifics (we work with &lt;a href='http://twitter.com/jadelisle'&gt;Josh Delisle&lt;/a&gt; and &lt;a href='https://twitter.com/kyork20'&gt;Kyle York&lt;/a&gt; - great guys). Note that those intending to use GSLB with Elastic Load Balancers will need the CDN Management service.&lt;/p&gt;

&lt;h3 id='closing_remarks'&gt;Closing remarks&lt;/h3&gt;

&lt;p&gt;Previously, operating a global infrastructure required significant overhead. This is where AWS really shines. Amazon now has four regions spread across three continents, and there&amp;#8217;s minimal overhead to distribute your platform across all of them. You just need to add a layer to route users to the closest one.&lt;/p&gt;

&lt;p&gt;The use of Amazon CloudFront in conjunction with a global EC2 infrastructure is a killer combo for improving application performance. And with Amazon continually expanding with new AWS regions, it&amp;#8217;s only going to get better.&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Cloudgrapher Launched</title>
   <link href="http://mbabineau.github.com/2010/04/27/cloudgrapher-launched"/>
   <updated>2010-04-27T00:00:00-07:00</updated>
   <id>http://mbabineau.github.com/2010/04/27/cloudgrapher-launched</id>
   <content type="html">&lt;p&gt;I&amp;#8217;ve launched a new service for graphing Amazon CloudWatch metrics, &lt;a href='https://www.cloudgrapher.com'&gt;Cloudgrapher&lt;/a&gt;. It&amp;#8217;s free, it&amp;#8217;s based off of &lt;a href='https://github.com/mbabineau/cloudviz'&gt;cloudviz&lt;/a&gt; and Google App Engine, and I think EC2 users (especially those with highly dynamic environments) will find it pretty handy.&lt;/p&gt;

&lt;p&gt;For more info, check out the &lt;a href='http://cloudgrapher.posterous.com/introducing-cloudgrapher'&gt;announcement&lt;/a&gt; on the &lt;a href='http://cloudgrapher.posterous.com/'&gt;Cloudgrapher blog&lt;/a&gt;. Or just head on over to &lt;a href='(https://www.cloudgrapher.com'&gt;http://www.cloudgrapher.com/&lt;/a&gt;) and give it a try! It&amp;#8217;s free and requires zero setup - what&amp;#8217;s not to like?&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Introducing Cloudviz</title>
   <link href="http://mbabineau.github.com/2010/03/17/introducing-cloudviz"/>
   <updated>2010-03-17T00:00:00-07:00</updated>
   <id>http://mbabineau.github.com/2010/03/17/introducing-cloudviz</id>
   <content type="html">&lt;p&gt;&lt;em&gt;(originally posted on Bizo&amp;#8217;s dev blog &lt;a href='http://dev.bizo.com/2010/03/introducing-cloudviz.html'&gt;here&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://aws.amazon.com/cloudwatch/'&gt;Amazon CloudWatch&lt;/a&gt; exposes a variety of useful metrics for EC2 instances, Elastic Load Balancers, and more. Unfortunately, it is tedious to query directly and the results can be difficult to interpret.&lt;/p&gt;

&lt;p&gt;Like most operational metrics, CloudWatch data provides the most insight when graphed. While there are existing tools to graph CloudWatch data, they are only available as part of a proprietary suite or service and, generally, they sacrifice customization and flexibility for ease-of-use.&lt;/p&gt;

&lt;p&gt;Here at Bizo, we wanted to incorporate CloudWatch data into operational dashboards. Nothing we found was flexible enough to meet our needs, so we decided to write our own. We are now releasing it to for all to use.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m pleased to introduce &lt;a href='http://github.com/mbabineau/cloudviz'&gt;cloudviz&lt;/a&gt;, an open source tool for creating embeddable CloudWatch graphs.&lt;/p&gt;

&lt;p&gt;Specifically, cloudviz is a data source that exposes CloudWatch data for graphing by &lt;a href='https://developers.google.com/chart/'&gt;Google Chart Tools&lt;/a&gt;. It&amp;#8217;s written in Python using Google&amp;#8217;s &lt;a href='https://developers.google.com/chart/interactive/docs/dev/gviz_api_lib'&gt;Data Source library&lt;/a&gt; and Mitch Garnaat&amp;#8217;s excellent AWS interface, &lt;a href='http://code.google.com/p/boto'&gt;boto&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With cloudviz, it&amp;#8217;s easy to create graphs like these: &lt;img alt='example host cpu' src='/img/cloudviz-example-hosts-cpu.png' /&gt; &lt;img alt='example elb request count' src='/img/cloudviz-example-elb-requestcount.png' /&gt;&lt;/p&gt;

&lt;p&gt;I encourage you to check out the project on GitHub &lt;a href='http://github.com/mbabineau/cloudviz'&gt;here&lt;/a&gt;. There&amp;#8217;s a fairly detailed README and plenty of examples, but feel free to drop me a line if you have any questions, &lt;a href='michael.babineau@gmail.com'&gt;michael.babineau@gmail.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Happy graphing!&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>SSH to EC2 Instance ID</title>
   <link href="http://mbabineau.github.com/2010/03/02/ssh-to-ec2-instance-id"/>
   <updated>2010-03-02T00:00:00-08:00</updated>
   <id>http://mbabineau.github.com/2010/03/02/ssh-to-ec2-instance-id</id>
   <content type="html">&lt;p&gt;I often find myself looking up EC2 nodes by instance ID so I can grab the external DNS name and SSH in. Fed up with the extra “ec2-describe-instance , copy, paste” layer, I threw together a function (basically a fancy alias) to SSH into an EC2 instance referenced by ID.&lt;/p&gt;

&lt;p&gt;Assuming you’re on Mac OS X / Linux, just put this somewhere in &lt;code&gt;~/.profile&lt;/code&gt;, reload your terminal, and you’re good to go. Alternatively, you can use the &lt;a href='https://gist.github.com/mbabineau/319882#file_ssh_instance.sh'&gt;shell script&lt;/a&gt; version.&lt;/p&gt;

&lt;p&gt;Update (3/5/10): Added region support&lt;/p&gt;

&lt;p&gt;Function version (latest source &lt;a href='https://gist.github.com/mbabineau/319882/raw/4116128eb09ebc293bfc111941dd1091671036b9/ssh-instance-function.sh'&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='k'&gt;function &lt;/span&gt;ssh-instance&lt;span class='o'&gt;()&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt; &lt;span class='nv'&gt;$# &lt;/span&gt;-lt 1 &lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;||&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt; &lt;span class='nv'&gt;$# &lt;/span&gt;-gt 3 &lt;span class='o'&gt;]&lt;/span&gt;; &lt;span class='k'&gt;then&lt;/span&gt;
&lt;span class='k'&gt;        &lt;/span&gt;&lt;span class='nb'&gt;echo&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;Usage: ssh-instance [-r region] &amp;lt;instance id&amp;gt;&amp;quot;&lt;/span&gt; 
    &lt;span class='k'&gt;else &lt;/span&gt;
&lt;span class='k'&gt;        case&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;$1&amp;quot;&lt;/span&gt; in
            &lt;span class='s2'&gt;&amp;quot;-r&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
                &lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt; &lt;span class='nv'&gt;$# &lt;/span&gt;-eq 3 &lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt; -n &lt;span class='s2'&gt;&amp;quot;`echo $3|egrep \&amp;quot;^i-[0-9a-z]+\&amp;quot;`&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;]&lt;/span&gt;; &lt;span class='k'&gt;then&lt;/span&gt;
&lt;span class='k'&gt;                    &lt;/span&gt;ssh &lt;span class='sb'&gt;`&lt;/span&gt;ec2-describe-instances --region &lt;span class='nv'&gt;$2&lt;/span&gt; &lt;span class='nv'&gt;$3&lt;/span&gt;|grep &lt;span class='s2'&gt;&amp;quot;^INSTANCE&amp;quot;&lt;/span&gt;|cut -f4&lt;span class='sb'&gt;`&lt;/span&gt;
                &lt;span class='k'&gt;else&lt;/span&gt;
&lt;span class='k'&gt;                    &lt;/span&gt;&lt;span class='nb'&gt;echo&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;Usage: ssh-instance [-r region] &amp;lt;instance id&amp;gt;&amp;quot;&lt;/span&gt;
                    &lt;span class='k'&gt;return &lt;/span&gt;1
                &lt;span class='k'&gt;fi&lt;/span&gt;;;
            i-&lt;span class='o'&gt;[&lt;/span&gt;0-9a-zA-Z&lt;span class='o'&gt;]&lt;/span&gt;*&lt;span class='o'&gt;)&lt;/span&gt;
                &lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt; &lt;span class='nv'&gt;$# &lt;/span&gt;-eq 3 &lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;$2&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;==&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;-r&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;]&lt;/span&gt;; &lt;span class='k'&gt;then&lt;/span&gt;
&lt;span class='k'&gt;                    &lt;/span&gt;ssh &lt;span class='sb'&gt;`&lt;/span&gt;ec2-describe-instances --region &lt;span class='nv'&gt;$3&lt;/span&gt; &lt;span class='nv'&gt;$1&lt;/span&gt;|grep &lt;span class='s2'&gt;&amp;quot;^INSTANCE&amp;quot;&lt;/span&gt;|cut -f4&lt;span class='sb'&gt;`&lt;/span&gt;
                &lt;span class='k'&gt;elif&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt; &lt;span class='nv'&gt;$# &lt;/span&gt;-eq 1 &lt;span class='o'&gt;]&lt;/span&gt;; &lt;span class='k'&gt;then&lt;/span&gt;
&lt;span class='k'&gt;                    &lt;/span&gt;ssh &lt;span class='sb'&gt;`&lt;/span&gt;ec2-describe-instances &lt;span class='nv'&gt;$1&lt;/span&gt;|grep &lt;span class='s2'&gt;&amp;quot;^INSTANCE&amp;quot;&lt;/span&gt;|cut -f4&lt;span class='sb'&gt;`&lt;/span&gt;
                &lt;span class='k'&gt;else&lt;/span&gt;
&lt;span class='k'&gt;                    &lt;/span&gt;&lt;span class='nb'&gt;echo&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;Usage: ssh-instance [-r region] &amp;lt;instance id&amp;gt;&amp;quot;&lt;/span&gt;
                    &lt;span class='k'&gt;return &lt;/span&gt;1
                &lt;span class='k'&gt;fi&lt;/span&gt;;;
            *&lt;span class='o'&gt;)&lt;/span&gt;  
                &lt;span class='nb'&gt;echo&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;Usage: ssh-instance [-r region] &amp;lt;instance id&amp;gt;&amp;quot;&lt;/span&gt; 
            &lt;span class='k'&gt;esac&lt;/span&gt;
&lt;span class='k'&gt;    fi&lt;/span&gt;

&lt;span class='k'&gt;    return &lt;/span&gt;0
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And the &lt;a href='https://gist.github.com/mbabineau/319882/raw/3cb3f1ea64e4e752e033ca472c04ee547fa042c3/ssh-instance.sh'&gt;script version&lt;/a&gt;&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Monitoring Amazon SQS Queue Length</title>
   <link href="http://mbabineau.github.com/2009/12/14/monitoring-amazon-sqs-queue-length"/>
   <updated>2009-12-14T00:00:00-08:00</updated>
   <id>http://mbabineau.github.com/2009/12/14/monitoring-amazon-sqs-queue-length</id>
   <content type="html">&lt;p&gt;At ShareThis, we use Amazon SQS for a number of core product features, most notably the delivery of email shares. When you use our widget to share something via email, our system creates a database record and logs a message in a queue. Queued items are asynchronously processed by our message sending service.&lt;/p&gt;

&lt;p&gt;We monitor this sending service in a variety of ways, but as any monitoring expert will tell you, it’s tough to anticipate every means by which something can break. Fortunately for us, most of our potential sending issues share a common symptom: the queue backs up.&lt;/p&gt;

&lt;p&gt;Amazon exposes SQS queue length through their API, but as there were no tools available for monitoring it, I wrote one in Python and made it available on GitHub: &lt;a href='https://github.com/mbabineau/check_sqs_queue'&gt;check_sqs_queue&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;check_sqs_queue can be run as a stand-alone script, emailing alert recipients directly through a configured SMTP server, or as a Nagios plugin. To run it, you must have the &lt;a href='http://code.google.com/p/boto/'&gt;boto&lt;/a&gt; library installed, and you must have &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; defined in &lt;code&gt;boto.cfg&lt;/code&gt; or a specified config file.&lt;/p&gt;

&lt;p&gt;Usage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;check_sqs_queue.py -q &amp;lt;queue name&amp;gt; [-w &amp;lt;warning threshold&amp;gt;] -c &amp;lt;critical threshold&amp;gt; [-n &amp;lt;recipient(s)&amp;gt;] [-f &amp;lt;config file]

Options:
-f FILE, --config=FILE
            configuration file
-q QUEUE, --queue=QUEUE
            Amazon SQS queue name (name only, not the URL)
-w WARN, --warning=WARN
            warning threshold
-c CRIT, --critical=CRIT
            critical threshold
-n RECIPIENT(s), --notify=RECIPIENT(s)
            comma-separated list of email addresses to notify&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By default, check_sqs_queue uses the boto config file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /etc/boto.cfg
[Credentials]
aws_access_key_id = 123456790ABCDEFGHIJ
aws_secret_access_key = 0987654321ZXYWVUTSRQPO123456789&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s see it in action:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ check_sqs_queue.py -q test_queue -c 50
Queue OK: &amp;quot;test_queue&amp;quot; contains 17 messages

$ check_sqs_queue.py -q test_queue -c 10
Queue CRITICAL: &amp;quot;test_queue&amp;quot; contains 17 messages

$ check_sqs_queue.py -q test_queue -w 10 -c 50
Queue WARNING: &amp;quot;test_queue&amp;quot; contains 17 messages&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By putting SMTP credentials into a specified config file, the script can alert a list of email recipients:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat check_sqs_queue.conf
[AWS]
aws_access_key_id = 123456790ABCDEFGHIJ
aws_secret_access_key = 0987654321ZXYWVUTSRQPO123456789

[SMTP]
smtp_server = smtp.gmail.com
smtp_port = 587
smtp_user = user@example.com
smtp_password = cleverpassword

$ check_sqs_queue.py -f check_sqs_queue.conf -q test_queue -w 5 \
-c 10 -n mike@example.com,joe@example.com,dan@example.com
Queue CRITICAL: &amp;quot;test_queue&amp;quot; contains 17 messages&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The resulting email:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Date: Mon, 14 Dec 2009 15:52:44 -0800 (PST)  
From: user@example.com  
Subject: Queue CRITICAL: &amp;quot;test_queue&amp;quot; contains 17 messages  
To: mike@example.com

&amp;quot;test_queue&amp;quot; contains 17 messages&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So go ahead and give it a try. If you run into any issues, please feel free to drop me a line: &lt;a href='mailto:michael.babineau@gmail.com'&gt;michael.babineau@gmail.com&lt;/a&gt;.&lt;/p&gt;</content>
 </entry>
 
 
</feed>